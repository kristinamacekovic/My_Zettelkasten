#dev #fcc #nlp #data #tensorflow #python
# Word Embeddings
There are 2 ways for a machine to represent a set of words. Starting from an initial text, we gather all unique words and:
## 1-hot encoding
Going through all the possible words, set 1 if it's that particular word, otherwise set to 0.
PRO: simple
CON: Inefficient (sparse), no connections between words

## Integer encoding
1. Order the words (i. e. assign _any_ number to each unique word)
2. Set number in array to the number of the unique word

PRO: more efficient than 1.
CON: still no way to represent connections between words (e. g. "King" and "Queen" are connected to each other)
**What's the mathematical theory behind this?** These vectors produce a basis for the "word space". They are orthogonal to each other, i. e. their dot product=0. In other words, there are no projections from one word to another (no _overlap_ between the spaces)

## Word Embeddings
They solve the above described problem (of not having connections between words). It keeps the integer encoding, but 
3. Adds a transformation to another space of _some_ dimension (a hyperparameter of training).
What does this mean in practice? E. g. word "King" transformed to a vector in space of dimension 8 is a vector with 8 _floating point_ values that describe it's association with other words. Taking the dot product of these types of vectors gives us a non-0 result, which expresses the connection between words.

### Example
Taking a bunch of reviews for a movie, we can train a model on the word embeddings in order to predict if a review is "good" or "bad". Moreover, we can see the word correlations as a result (that add to the final decision).

# Resources
“Intro to NLP,” n.d. https://www.freecodecamp.org/news/learn-natural-language-processing-no-experience-required/.



