#data #machinelearning #gradient_descent

One of the hyperparameters of gradient descent is the _learning rate_. It is the fraction we mutiply the gradents magnitude with in order to step to the next point in the algorithm.


* a learning rate that is too big will overshoot the minimum and bounce around maybe forever
* a learning rate that is too small will take very long to get to the minimum

For every regression problem, there is a _goldilocks_ (just right amount) learning rate. It is related to how flat the loss function is. In case of a very flat function, the learning rate and therefore step size can be bigger.

## The ideal learning rate
* for a 1D function: $\frac{1}{f''(x)}$ (inverse of the 2nd derivative with respect to x)
* 2+D: the inverse of the Hessian (matrix of 2nd partial derivatives)

**NOTE:** In practice, finding a "perfect" (or near-perfect) learning rate is not essential for successful model training. The goal is to find a learning rate large enough that gradient descent converges efficiently, but not so large that it never converges.



# Resource
[@http://zotero.org/users/local/l0LufOnX/items/CNIKLSZG]

# Links
[[20200212100944]] 20200212100944 - gradient descent
[[20200212093832]] 20200212093832 - loss
